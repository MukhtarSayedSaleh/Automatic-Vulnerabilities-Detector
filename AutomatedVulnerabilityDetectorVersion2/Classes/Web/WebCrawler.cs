/*
 * Web Crawler (Spider) Class
 * the main purpose of this class is to scan
 * a website (web pages actually) and reutrns
 * the links of it's pages.
 * 
 *       Mukhtar Sayed Saleh
 *       Syria- Al_Boukamal
 *       mokhtar_ss@hotmail.com
 *       00963944467547   
 *       
 *   start date : 29-8-2010
 *   finish date : 13-9-2010 due to regular expression errors
 *                           which have been fixed later !!!!
 *   testing start date :13-9-2010
 *   testing finish date : 9-10-2010 // OK After some modifications was in the conversion 
 *                                      between relative and absolute urls!!!
 */

using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;

// regular expression is the best 
// solution to find the elements 
// inside html markup!.
using System.Text.RegularExpressions;
using System.Windows.Forms;
using System.Net;
using System.IO;

using AutomatedVulnerabilityDetectorVersion2.GUIs;

namespace AutomatedVulnerabilityDetectorVersion2.Classes.Web
{
    class WebCrawler
    {

        // class internal vars.
        protected List<string> links = null;
        protected string baseURL; // target URL.

        /// <summary>
        /// Default consructor wich initialize the Regular Expressions .... etc.
        /// </summary>
        public WebCrawler(string target)
        {
            this.baseURL = target;
            links = new List<string>();
            //errors = new List<string>();
        }

        /// <summary>
        /// add a new error to my errors log.
        /// </summary> 
        protected void displayOutput(string p)
        {
            try
            {
                SharedVariables.myTestingForm.displayOutputActivity(p);
            }
            catch (Exception) { }
        }

        
        /// <summary>
        /// fetches web page and returns its html markup as string !.
        /// </summary>
        public string fetchPage()
        {
            
            string strURL = this.baseURL;
            HttpWebRequest req = null;
            try
            {
                req = HttpWebRequest.Create(strURL) as HttpWebRequest;
            }
            catch (Exception ex)
            {
                displayOutput(string.Format("getting page : {0} fails , details : {1} \n", strURL, ex.Message));
            }

            // unable to create request object !!!
            if (req == null)
            {
                displayOutput(string.Format("cann't create request object for page : {0} \n", strURL));
                return string.Empty;
            }

            req.Method = "GET";

            HttpWebResponse res = null;
            try
            {
                res = req.GetResponse() as HttpWebResponse;
            }
            catch (Exception ex)
            {
                displayOutput(string.Format("No response , url : {0} , details : {1} \n",
                    strURL, ex.Message));
            }

            if (res != null && res.StatusCode != HttpStatusCode.OK)
            {
                displayOutput(string.Format("error while retrieving : {0} , server response : {1} \n",
                    strURL, res.StatusCode));
            }

            // Unknown error !!!!!!!
            if (res == null || res.StatusCode != HttpStatusCode.OK)
            {
                return string.Empty;
            }

            Stream s = res.GetResponseStream();

            StreamReader sr = new StreamReader(s);

            //Read the whole content of the response stream into a string
            string strHTML = sr.ReadToEnd();

            sr.Close();
            sr.Dispose();
            sr = null;

            s.Close();
            s.Dispose();
            s = null;


            displayOutput(string.Format("Fetched successfully , url : {0} \n", strURL));
            links.Add(strURL);
            return strHTML;
        }
       
        /// <summary>
        /// finds all links in strHTML (html markUp.)
        /// and adds it to the link list.
        /// </summary>
        public void analysePage(string strHTML)
        {
             HtmlParser p = new HtmlParser(this.baseURL, strHTML);
             links.AddRange(p.getInternalLinks());
        }

        /// <summary>
        /// returns all links;
        /// </summary>
        public List<string> getLinks()
        {
            return this.links;
        }


    }
}
